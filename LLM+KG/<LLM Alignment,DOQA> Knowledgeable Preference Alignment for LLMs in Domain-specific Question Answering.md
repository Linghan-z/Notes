# Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering

## abstract

1. user-friendly
2. domain knowledge

è®¤ä¸ºä»¥ä¸Šä¸¤ä¸ªéœ€æ±‚éƒ½å¯ä»¥è®¤ä¸ºæ˜¯model **preference** é—®é¢˜ï¼Œéœ€è¦ä¸äººç±»**å¯¹é½**

**Know**ledgeable **P**reference **A**lignmen**T** **(KnowPAT)**ï¼š

- preference setï¼š

  - style preference set

  - knowledge preference set

## 1 Introduction

**how can LLM be used to solve real-scenario QA problems that are supported by external knowledge graphs?**

**æ–‡ç« ï¼šGraph Neural Prompting with Large Language Models.**

- é¦–å…ˆä¸ºé—®é¢˜æ£€ç´¢ç›¸å…³çŸ¥è¯†ä¸‰å…ƒç»„ä½œä¸ºå‚è€ƒçŸ¥è¯†ï¼Œç„¶åé€šè¿‡åŒ…å«è¯¥çŸ¥è¯†çš„æç¤ºå¯¹LLMè¿›è¡Œå¾®è°ƒ
- é—®é¢˜ï¼š
  1. ç”±LLMsç”Ÿæˆçš„ç­”æ¡ˆéœ€è¦ç”¨æˆ·å‹å¥½ï¼Œå¹¶é¿å…ç”Ÿæˆä¸å½“å†…å®¹ï¼Œå¦‚ä¸å‹å¥½ã€ä½è´¨é‡çš„å›ç­”
  2. æ£€ç´¢åˆ°çš„æ–‡ç« ä¸æ˜¯éƒ½æœ‰ç”¨
     1. <img src="./assets/CleanShot 2024-03-15 at 14.45.52@2x.png" alt="CleanShot 2024-03-15 at 14.45.52@2x" style="zoom:50%;" />

æå‡º==**Know**ledgeable **P**reference **A**lignmen**T** **(KnowPAT)**==ï¼š

- preference setï¼š

  - **style preference** set

  - **knowledge preference** set

- **knowledgeable preference set construction**ï¼šé¢†åŸŸçŸ¥è¯†å›¾è°±æ•´åˆåˆ°æ„å»ºçŸ¥è¯†åå¥½æ•°æ®é›†

**contributions**ï¼š

1. preference alignment for é¢†åŸŸé—®ç­”ï¼Œä½¿ç”¨LLMå’Œé¢†åŸŸKG
2. knowledgeable prefernece alignment çŸ¥è¯†åå¥½å¯¹é½æ¡†æ¶

## 2 Related Works

### 2.1 KG-enhanced Question Answering

åˆ©ç”¨æç¤ºä¸ï¼ˆå¤§å‹ï¼‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯ä»¥ä¿ƒè¿›è·¯å¾„æ¨ç†å’Œç»†åŒ–çŸ¥è¯†å›¾æ£€ç´¢èŒƒå›´:

- [11] Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Z. Pan, Ningyu Zhang, and Wen Zhang. 2022. LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. In IJCKG. ACM, 20â€“29.
- [26] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: A General Framework for Large Language Model to Reason over Structured Data. CoRR abs/2305.09645 (2023). 
- [27] Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023. UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. In ICLR. OpenReview.net.
- [38] Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, and Wei Lin. 2023. ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. CoRR abs/2310.08975 (2023).
- [76] Lingxi Zhang, Jing Zhang, Yanling Wang, Shulin Cao, Xinmei Huang, Cuiping Li, Hong Chen, and Juanzi Li. 2023. FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering. In ACL (1). Association for Computational Linguistics, 1002â€“1017.

### 2.2 Preference Alignment for LLMs

## 3 PROBLEM SETTING

- **ç›®æ ‡**ï¼šä½¿ç”¨QAæ•°æ®é›†$\mathcal{D} = \{(ğ‘_ğ‘–, ğ‘_ğ‘– ) | ğ‘– = 1, 2, . . . , ğ‘ \}$å¾®è°ƒLLM$\mathcal{M}$ 
  - qiå’Œaiæ˜¯QAå¯¹
- **Vanilla fine-tuning**ï¼šé¦–å…ˆç”¨æç¤ºæ¨¡æ¿$\mathcal{I}$åŒ…è£…QAå¯¹ï¼Œæ¨¡å‹$\mathcal{M}$ é€šè¿‡autoregressive lossè¿›è¡Œä¼˜åŒ–ï¼Œè¡¨ç¤ºä¸ºï¼š
  - <img src="./assets/CleanShot 2024-03-15 at 15.54.16@2x.png" alt="CleanShot 2024-03-15 at 15.54.16@2x" style="zoom:50%;" />
    - aijï¼šç­”æ¡ˆaiçš„ç¬¬jä¸ªtoken
    - $P_\mathcal{M}$ï¼šæ¨¡å‹$\mathcal{M}$é¢„æµ‹çš„tokneçš„æ¦‚ç‡
  - è¿™æ˜¯LLMsçš„åŸºæœ¬è°ƒæ•´æ–¹æ³•ã€‚æœ‰äº†è¿™æ ·ä¸€ä¸ªè®­ç»ƒç›®æ ‡ï¼Œè®­ç»ƒQAæ•°æ®å°±ä½œä¸ºç›‘ç£ä¿¡æ¯æ¥è°ƒæ•´æ¨¡å‹$\mathcal{M}$ä»¥é€‚åº”QAåœºæ™¯
- **domain-specific task**:åŸºäºæ–‡æ¡£æ„å»º**cloud product knowledge graph (CPKG)**$\mathcal{G}=(\mathcal{E},\mathcal{R},\mathcal{T})$ 
  - åˆ†åˆ«æ˜¯ï¼šå®ä½“é›†åˆã€å…³ç³»é›†åˆã€ä¸‰å…ƒç»„é›†åˆ
  - é€šè¿‡æ£€ç´¢ç›¸å…³æ€§æ›´é«˜çš„å‰kä¸ªçŸ¥è¯†ï¼Œè¾“å…¥æç¤ºå°†åŒ…å«å·²æ£€ç´¢åˆ°çš„çŸ¥è¯†$\mathcal{K}$
  - å› æ­¤ï¼Œ$\mathcal{M}$å¯ä»¥åœ¨VFTè¿‡ç¨‹ä¸­å­¦ä¹ ç›¸å…³çŸ¥è¯†ï¼Œè¿™æ˜¯ç”¨äºç‰¹å®šé¢†åŸŸLLMåº”ç”¨çš„é€šç”¨æµç¨‹

ä»¥ä¸Šçš„è¿™ä¸ªè¿‡ç¨‹ä¸èƒ½æœ‰è‰¯å¥½çš„ç»“æœï¼š

1. åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨åº”è¯¥ç”¨æˆ·å‹å¥½ï¼Œå¦åˆ™å®ƒä»¬å°†æ— æ³•å¸¦æ¥å•†ä¸šä»·å€¼ï¼Œå› æ­¤è¾“å‡ºè¦ç”¨æˆ·å¯æ¥å—
2. çŸ¥è¯†æ£€ç´¢è¿‡ç¨‹æ˜¯**æ— ç›‘ç£çš„ï¼Œéš¾ä»¥ä¿è¯æ£€ç´¢åˆ°çš„çŸ¥è¯†çš„æœ‰æ•ˆæ€§**ï¼Œè¿™æ„å‘³ç€æ¨¡å‹$\mathcal{M}$éœ€è¦è·å¾—åˆ¤æ–­å’Œé€‰æ‹©æ€§åˆ©ç”¨çŸ¥è¯†ä¸‰å…ƒç»„çš„èƒ½åŠ›

**apply preference alignment during LLM fine-tuning** å¾®è°ƒåº”ç”¨åå¥½å¯¹é½ï¼š

- preference setï¼š$\mathcal{P} = \{b_1, b_2, . . . , b_ğ‘™ \}$  æ¯ä¸ªQaå¯¹çš„lä¸ªä¸åŒçš„ç­”æ¡ˆã€‚æˆ‘ä»¬å°† ğ‘Ÿğ‘– è¡¨ç¤ºä¸ºæ¯ä¸ªç­”æ¡ˆ ğ‘ğ‘– çš„åå¥½åˆ†æ•°ï¼Œå…¶ä¸­è¾ƒé«˜çš„ ğ‘Ÿğ‘– è¡¨ç¤ºäººç±»æ›´å–œæ¬¢è¿™ä¸ªç­”æ¡ˆ
- è®­ç»ƒè¿‡ç¨‹ä¸­å®šä¹‰å¦ä¸€ä¸ªç›®æ ‡$\mathcal{L}_{align}$ ï¼Œè®©æ¨¡å‹Mä¸åå¥½é›†åˆPå¯¹é½ï¼š<img src="./assets/CleanShot 2024-03-15 at 16.08.00@2x.png" alt="CleanShot 2024-03-15 at 16.08.00@2x" style="zoom:50%;" />

åœ¨è¿™æ ·ä¸€ä¸ªå¤šä»»åŠ¡ç›®æ ‡ä¸‹ï¼Œæ¨¡å‹Mè¢«å¾®è°ƒä»¥é€‚åº”é»„é‡‘ç­”æ¡ˆï¼ŒåŒæ—¶é¿å…ä¸å—æ¬¢è¿çš„ç»“æœ

## 4 KNOWLEDGEABLE PREFERENCE ALIGNMENT PIPELINE

KnowPATçš„æµç¨‹ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ— ç›‘ç£ä¸‰å…ƒç»„linkingï¼ŒçŸ¥è¯†åå¥½é›†æ„å»ºï¼Œå¾®è°ƒå’Œè®­ç»ƒ

<img src="./assets/CleanShot 2024-03-15 at 16.17.08@2x.png" alt="CleanShot 2024-03-15 at 16.17.08@2x" style="zoom:50%;" />

### 4.1 Unsupervised Triple Linking

- å°†CPKG $\mathcal{G}$ä¸­çš„ä¸‰å…ƒç»„ä¸æ¯ä¸ªé—®é¢˜ ğ‘ğ‘– ç›¸å…³è”
  - ç®€å•çš„åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„retrieer$\mathcal{H}$
    - $sim(i, j) = Cosine(H(q_i ), H (h_j , r_j , t_j ))$
    - H ä½œä¸ºtextual encoder
    - ç›¸ä¼¼æ€§åŸºäºä½™å¼¦å‡½æ•°
    - ä¸ºæ¯ä¸ªé—®é¢˜qiæ£€ç´¢top-kä¸ªä¸‰å…ƒç»„
    - æ£€ç´¢çš„çŸ¥è¯†(RK) as $\mathcal{K}$
    - RKå°†è¢«æ·»åŠ åˆ°è¾“å…¥æç¤ºä¸­ï¼Œä½œä¸ºå½“å‰é—®é¢˜çš„èƒŒæ™¯çŸ¥è¯†
    - **è¿‡ç¨‹æ˜¯æ— ç›‘ç£çš„ï¼Œå¹¶ä¸”éœ€è¦å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œ**ä½†æ˜¯æ£€ç´¢åˆ°çš„çŸ¥è¯† $\mathcal{K}$ å¯èƒ½å­˜åœ¨å™ªéŸ³ï¼Œå¹¶ä¸”æ— æ³•æä¾›è§£å†³é—®é¢˜æ‰€éœ€çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå› æ­¤LLMéœ€è¦**å­¦ä¹ çŸ¥è¯†åå¥½**ï¼Œä»**æ£€ç´¢åˆ°çš„çŸ¥è¯† $\mathcal{K}$ä¸­é€‰æ‹©æœ‰ç”¨çš„ä¿¡æ¯**ã€‚

### 4.2 Knowledgeable Preference Set Construction

the style preference set $\mathcal{P}_s$ and the knowledge preference set  $\mathcal{P}_k$

- style preference set (SPS) $\mathcal{P}_s$ 
  - é€‰æ‹©l âˆ’ 1ä¸ªLLMï¼ˆæ€§èƒ½ä¸ä¸€ï¼Œä¸”ä¸å¦‚golden answerï¼‰ï¼Œä¸äººå·¥æ ‡è®°çš„goldenç­”æ¡ˆç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸ºlçš„**style preference set$\mathcal{P}_s=\{b_1,b_2,...,b_l\}$** 
- knowledge preference set (KPS) $\mathcal{P}_k$ 
  - æˆ‘å‡è®¾å…·æœ‰é«˜ç›¸ä¼¼åº¦ä½†æœªè¾¾åˆ°å‰top-kçš„ä¸‰å…ƒç»„æ›´å¯èƒ½æ˜¯å¯¹è¾“å…¥é—®é¢˜æ— ç”¨çš„çŸ¥è¯†ï¼Œé€šè¿‡æ£€ç´¢ä¸€äº›ç›¸å¯¹è¾ƒå·®çš„çŸ¥è¯†å¹¶ä¿ƒä½¿æ¨¡å‹ç”Ÿæˆå…·æœ‰ä¸åŒè´¨é‡çŸ¥è¯†çš„å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—å…·æœ‰ä¸åŒè´¨é‡çš„åå¥½é›†
  - æ£€ç´¢ä¸‰ç»„ä¸‰å…ƒç»„ $\mathcal{K}_1,\mathcal{K}_2,\mathcal{K}_3$ 
    - $\mathcal{K}_1$:æ£€ç´¢çš„top-kä¸‰å…ƒç»„
    - $\mathcal{K}_2=\empty$:ç©ºé›†
    - $\mathcal{K}_3$ï¼štop k + 1 to 2kï¼Œå®¹æ˜“è¯¯ç”¨çš„çŸ¥è¯†ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦ç›¸å¯¹è¾ƒé«˜
  - ç»™LLMç”Ÿæˆä¸åŒçš„ç­”æ¡ˆ
    - è¿™äº›ç”Ÿæˆçš„3ä¸ªç­”æ¡ˆå’Œé»„é‡‘ç­”æ¡ˆå½¢æˆäº†ä¸€ä¸ªé£æ ¼åå¥½é›†åˆ$\mathcal{P}_s=\{c_1,c_2,c_3,c_4\}$ 
- é€šè¿‡è¿™æ ·åšï¼Œå¯ä»¥ä¸ºæ¯ä¸ªQAå¯¹è·å¾—ä¸¤ä¸ªåå¥½é›†
  - è®©l=4ï¼Œé›†åˆå¤§å°ç›¸ç­‰
  - è®¾è®¡äº†ä¸€ç§åŸºäºè§„åˆ™çš„ç­–ç•¥æ¥å†³å®šæ¯ä¸ªç­”æ¡ˆçš„åå¥½åˆ†æ•° ğ‘Ÿ
    - **style preference set $\mathcal{P}_s$**:é«˜è´¨é‡çš„é»„é‡‘ç­”æ¡ˆ ğ‘1 è¢«åˆ†é…äº†æœ€é«˜åˆ†æ•°ï¼Œå¹¶ä¸”å…¶ä»–LLMçš„ç­”æ¡ˆæ ¹æ®å®ƒä»¬çš„ä¸€èˆ¬èƒ½åŠ›ç¡®å®š
      - ChatGPT > ChatGLM > Vicuna
      - $r_1 > r_2 > r_3 > r_4$
    - **knowledge preference set $\mathcal{P}_k$**:golden answer c1 åˆ†æœ€é«˜ï¼Œtop-kç¬¬äºŒc2ï¼Œæ²¡æœ‰ç”¨çŸ¥è¯†c3ï¼Œè¯¯ç”¨çš„æ— å…³çŸ¥è¯†c49åœ¨æˆ‘ä»¬çš„å®é™…æµ‹è¯•ä¸­å‘ç°ï¼Œæ£€ç´¢åˆ°çš„çŸ¥è¯†K3ä¸é—®é¢˜ğ‘ä¹‹é—´çš„ä¸åŒ¹é…ç‡éå¸¸é«˜ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“è¯¯å¯¼æ¨¡å‹Mï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶å¾—åˆ†è®¾ç½®ä¸ºä½äºç©ºçŸ¥è¯†K2æƒ…å†µã€‚ï¼‰
      - $r_1 > r_2 > r_3 > r_4$
  - ä¿æŒè¿™ç§ç›¸å¯¹é¡ºåºå°†ç®€åŒ–ä»£ç çš„å®ç°ã€‚
- å¯¹äºæ¯ä¸ªQAå¯¹ï¼Œæ„å»ºä¸¤ä¸ªåå¥½é›†ï¼Œå¹¶æœ€ç»ˆè·å¾—åŒ…å«2ğ‘åå¥½é›†çš„**æ•´ä½“åå¥½æ•°æ®**ï¼Œåå¥½æ•°æ®å‚ä¸å¾®è°ƒè¿‡ç¨‹ä»¥æ§åˆ¶æ¨¡å‹$\mathcal{M}$çš„é£æ ¼åå¥½å’ŒçŸ¥è¯†åå¥½

### 4.3 Fine-tuning and Preference Alignment

è®¾è®¡äº†ä¸€ä¸ªåˆ†æ•°ï¼Œè¡¨ç¤ºåå¥½ï¼š

<img src="./assets/CleanShot 2024-03-15 at 18.03.30@2x.png" alt="CleanShot 2024-03-15 at 18.03.30@2x" style="zoom:50%;" />

- åˆ†æ•°Siæ¯ä¸ªç­”æ¡ˆçš„æ˜¯**å¹³å‡å¯¹æ•°ä¼¼ç„¶**ï¼Œæ¡ä»¶æ˜¯ç»™å®šçš„æç¤ºæ¨¡æ¿$\mathcal{I}$å’Œé—®é¢˜æŸ¥è¯¢qi

ä¸ºäº†ä½¿æ¨¡å‹åå¥½ä¸æˆ‘ä»¬çš„è®¾æƒ³ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„åœºæ™¯è®¾è®¡äº†ä¸€ä¸ª**æ–°çš„å¯¹é½ç›®æ ‡**ï¼Œè¡¨ç¤ºä¸ºï¼š

<img src="./assets/CleanShot 2024-03-15 at 18.08.07@2x.png" alt="CleanShot 2024-03-15 at 18.08.07@2x" style="zoom:50%;" />

- å®ç°åå¥½å¯¹é½è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å¯¹æ¯”äº†preferredç­”æ¡ˆå’Œunpreferredç­”æ¡ˆ
  - **noticeï¼š**äººç±»åå¥½è¯„åˆ† ğ‘Ÿğ‘– ä»…ç¡®å®šä¸ä¸åŒç­”æ¡ˆå¯¹åº”çš„æ’åºï¼Œè€Œä¸ä¼šç›´æ¥å‚ä¸è®¡ç®—å’Œæ¢¯åº¦ç´¯ç§¯
  - ç°å­˜çš„æ–¹æ³•RRHF [71] and SLiC-HF [83]ä½¿ç”¨Margin-rank lossï¼š<img src="./assets/CleanShot 2024-03-15 at 18.14.53@2x.png" alt="CleanShot 2024-03-15 at 18.14.53@2x" style="zoom:33%;" />å®ç°åå¥½å¯¹é½ï¼Œä½†æ˜¯ä»–ä»¬çš„è®¾è®¡åªæœ‰åœ¨äººç±»åå¥½ç­”æ¡ˆçš„æ¨¡å‹åå¥½åˆ†æ•°Sä½äºä¸å—æ¬¢è¿ç­”æ¡ˆæ—¶æ‰è¿›è¡Œä¼˜åŒ–ï¼ˆæ›´æ­£å¼çš„è¡¨è¿°ä¸ºå½“ğ‘Ÿğ‘— < ğ‘Ÿğ‘– æ—¶ï¼ŒSğ‘– < Sğ‘—ï¼‰ã€‚
  - æœ¬æ–‡è®¤ä¸ºï¼Œåœ¨åº”è¯¥ä»¥æŒç»­é™ä½ä¸å—æ¬¢è¿ç­”æ¡ˆçš„å‘ç”Ÿæ¦‚ç‡
- åŒæ—¶ï¼Œç”±äºä¸åŒç­”æ¡ˆå…·æœ‰ä¸åŒçš„æ–‡æœ¬è´¨é‡å’Œåå¥½ç¨‹åº¦ï¼Œè¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”æƒé‡æ¥æ§åˆ¶æ¯ä¸ªé¦–é€‰ç­”æ¡ˆçš„å½±å“ï¼Œè¯¥è‡ªé€‚åº”æƒé‡è¡¨ç¤ºä¸ºï¼š
  - <img src="./assets/CleanShot 2024-03-15 at 18.17.34@2x.png" alt="CleanShot 2024-03-15 at 18.17.34@2x" style="zoom:50%;" />
  - Sğ‘šğ‘ğ‘¥å’ŒSğ‘šğ‘–ğ‘›åˆ†åˆ«æ˜¯åå¥½é›†åˆPä¸­çš„æœ€å¤§å’Œæœ€å°æ¨¡å‹åå¥½åˆ†æ•°
  - å…·æœ‰è¿™æ ·ä¸€ç§è‡ªé€‚åº”æƒé‡ï¼Œä¸åŒåå¥½ç­”æ¡ˆçš„å½±å“å¯ä»¥åŠ¨æ€è°ƒæ•´ã€‚ç„¶åå¯¹é½æŸå¤±å˜ä¸ºï¼š
    - <img src="./assets/CleanShot 2024-03-15 at 18.18.16@2x.png" alt="CleanShot 2024-03-15 at 18.18.16@2x" style="zoom:50%;" />
  - æœ€ç»ˆçš„è®­ç»ƒç›®æ ‡ä»ç„¶æ˜¯ä»¥å¤šä»»åŠ¡æ–¹å¼è¿›è¡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªè¶…å‚æ•° ğœ† ä½œä¸ºå¯¹é½æŸå¤±çš„ç³»æ•°ï¼š
    - <img src="./assets/CleanShot 2024-03-15 at 18.18.40@2x.png" alt="CleanShot 2024-03-15 at 18.18.40@2x" style="zoom:50%;" />
    - P âˆ’ 1ä»£è¡¨äº†åå¥½-ä¸åå¥½å¯¹æ¯”çš„è®¡æ•°ï¼Œä»¥è§„èŒƒåŒ–å¯¹é½æŸå¤±ã€‚
  - é’ˆå¯¹åœ¨å‰ä¸€èŠ‚æ„å»ºçš„æ¯ä¸ªåå¥½é›†åˆï¼Œæ¨¡å‹éƒ½ä¼šæ ¹æ®è¿™æ ·ä¸€ä¸ªç›®æ ‡è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚

## 5 EXPERIMENT SETTINGS

### 5.1 Dataset Information

- CPKGï¼šwhich consists of 13995 entities, 463 relations, and 20752 triples
- The QA dataset consists of 8909 QA pairs and we split the dataset into 7909/500/500 for training/validation/test.
- For each data instance in the training, we construct two preference sets and finally get 15818 preference sets with 4 answers in each set.

### 5.2 Baseline Methods

1. Zero-shot approachï¼šdirectly prompts the LLM
2. In-context learning approach ï¼šsample a few (ğ‘˜-shot) QA pairs as demonstrations from the training dataset
3. Vanilla fine-tuning approachï¼šfine-tunes the LLM using the QA pairs with retrieved knowledge as Equation 1
4. Preference alignment approachesï¼šadditional preference alignment objectives during training
5. five existing state-of-the-art (SOTA)
   1. RRHF [71]
   2. SLiC-HF [83]
   3. PRO [48]
   4. AFT (both AFT-BC and AFT-DC) [58]

### 5.3 Evaluation Metrics

1. Traditional text generation metrics
   1. BLEU [43], ROUGE [32], CIDEr [56], and METEOR [3]â€”â€”â€”â€”ä¸èƒ½æä¾›è¯­æ„ä¸Šçš„ç›¸å…³æ€§å’Œæ·±å…¥çš„ç†è§£
2. Model-based metrics.
   1. BERTScore [78]ï¼šBertè®¡ç®—è¯­æ„ç›¸ä¼¼åº¦
   2. perplexity (PPL)ï¼šLLMçš„èƒ½åŠ›ç†è§£å¹¶é¢„æµ‹æ•´ä¸ªå¥å­
   3. preference scoreï¼šS mentioned in Equation 4ï¼Œååº”æ¨¡å‹çš„preference degree
      1. <img src="./assets/CleanShot 2024-03-15 at 18.25.50@2x.png" alt="CleanShot 2024-03-15 at 18.25.50@2x" style="zoom:25%;" />

### 5.4 Implementation Details

- **LLMï¼š**Atom-7Bï¼šfully open-source version of Llama2 with Chinese vocabulary extension
- **Retrieverï¼š**BGE-base-zh-v1.5 [67]

## 6 RESULTS ANALYSIS

- RQ1: How does the proposed method KnowPAT **perform** compared with the baseline methods? â€”â€”â€”â€”performance
- RQ2: Do the proposed **modules** in KnowPAT really benefit the performance of KnowPAT? â€”â€”â€”â€”design soundness
- RQ3: Are there some intuitive cases to demonstrate the **effectiveness** of KnowPAT? â€”â€”â€”â€”intuition
- RQ4: Does the LLM still keep the **general ability** rather than catastrophic forgetting? â€”â€”â€”â€”usability in real scenatios

### 6.1 Main Results (Q1)

  <img src="./assets/CleanShot 2024-03-15 at 18.32.36@2x.png" alt="CleanShot 2024-03-15 at 18.32.36@2x" style="zoom:50%;" />

<img src="./assets/CleanShot 2024-03-15 at 19.31.41@2x.png" alt="CleanShot 2024-03-15 at 19.31.41@2x" style="zoom:50%;" />

<img src="./assets/CleanShot 2024-03-15 at 19.33.28@2x.png" alt="CleanShot 2024-03-15 at 19.33.28@2x" style="zoom:50%;" />

- more acceptable to humans

### 6.2 Ablation Study (Q2)

fine-tuning objective$\mathcal{L}_{ft}$ and the alignment objective $\mathcal{L}_{align}$ are both contributing for the model performance

cloud product knowledge graph (CPKG)

- w/o RK removes the retrieved knowledge during the fine-tuning and preference alignment process
- w/o KG without KG in the whole processï¼Œ**KPS(knowledge preference setting)** and RK (retrieved knowledge) in the input prompt are all removed
- åˆ†æï¼šCPKG does not only serve as an **external knowledge source during training** but also participates in the **preference set construction process**, which is **important** to the model performance.

<img src="./assets/CleanShot 2024-03-15 at 19.58.35@2x.png" alt="CleanShot 2024-03-15 at 19.58.35@2x" style="zoom:50%;" />

### 6.3 Case Study (Q3)

<img src="./assets/CleanShot 2024-03-15 at 19.59.02@2x.png" alt="CleanShot 2024-03-15 at 19.59.02@2x" style="zoom:50%;" />

- similar to the golden answer
- keeping a user-friendly tone and providing sufficient informationï¼ˆç¬¬äºŒä¸ªï¼‰
- **retrieved knowledge in the first case is** (EIP, used for, IP Binding), (Select Box, belongs to, Alarm Management Component),éƒ½æ²¡ç”¨ï¼Œä½†æ˜¯**æ²¡æœ‰è¢«è¯¯å¯¼**

### 6.4 Knowledge Retention Analysis (Q4)

<img src="./assets/CleanShot 2024-03-15 at 20.03.36@2x.png" alt="CleanShot 2024-03-15 at 20.03.36@2x" style="zoom:50%;" />

- five distinctive commonsense regions (history, clinicalï¼ˆä¸´åºŠï¼‰, politics, computer science, and economics)
- å‡ºäº†ä¸´åºŠï¼Œåˆ«çš„éƒ½è¿˜æŒºå¥½çš„
- PROåœ¨ç»æµé—®é¢˜ä¸Šæ„å¤–åœ°æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ï¼Œä½†åœ¨å…¶ä»–å‡ ä¸ªé¢†åŸŸæ¯”KnowPATè¡¨ç°æ›´æ˜æ˜¾çš„æ€§èƒ½ä¸‹é™

## 7 CONLUSION

In this paper, we introduce a novel framework, **knowledgeable preference alignment (KnowPAT)**, for **domain-specific QA tasks** in cloud product services, **leveraging LLMs and KGs in a practical application setting**. Our approach **constructs a knowledgeable preference set by retrieving** and **utilizing knowledge triples to generate answers with different quantities**. **A new alignment objective** is designed to unleash the power of the **preference set**. Comprehensive experiments demonstrate that our method surpasses existing solutions for this **real-world challenge**. Looking ahead, we aim to apply KnowPAT to additional business scenarios and further investigate the potential of KG-enhanced LLMs.