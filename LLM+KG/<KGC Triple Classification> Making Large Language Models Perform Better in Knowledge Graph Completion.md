# Making Large Language Models Perform Better in Knowledge Graph Completion

> è¿™ç¯‡æ–‡ç« å‡†å¤‡ç²—è¯»ä¸€ä¸‹ï¼Œåªåšäº†ä¸€ä¸ªä¸‰å…ƒç»„çš„åˆ†ç±»ï¼Œåˆ©ç”¨çš„ç­–ç•¥æ¦‚æ‹¬ä¸€ä¸‹å°±æ˜¯æŠŠå›¾è°±çš„ç»“æ„çš„embeddingæ˜ å°„ç„¶åæ”¾åœ¨promptçš„å‰é¢ä½œä¸ºä¸€ä¸ªå‰ç¼€ï¼Œé€šè¿‡å¾®è°ƒè®©LLMå­¦ä¹ åˆ°å›¾è°±çš„ç»“æ„ä¿¡æ¯
>
> é‡ç‚¹æ˜¯å¯¹äºå›¾è°±çš„**ç»“æ„ä¿¡æ¯**çš„å¤„ç†

1. first transfer the existing LLM paradigms to structural-aware settings and further propose a **kn**owledge **p**refix **a**dapter (**KoPA**)
2. KoPA employs **structural embedding pre-training** to capture the structural information of entities and relations in the KG.
3. KoPA **informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt**

#### embedding-based KGC

åŸºäºåµŒå…¥çš„æ–¹æ³•[4, 43, 46, 56]æ—¨åœ¨å°†çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œå…³ç³»åµŒå…¥åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­è¡¨ç¤ºç©ºé—´ä¸­ã€‚è¿™äº›æ–¹æ³•å……åˆ†åˆ©ç”¨äº†çŸ¥è¯†å›¾è°±ä¸­çš„ç»“æ„ä¿¡æ¯ï¼Œé€šè¿‡è®¾è®¡è‰¯å¥½çš„è¯„åˆ†å‡½æ•°æ¥å»ºæ¨¡ä¸‰å…ƒç»„å¯ä¿¡åº¦ï¼Œå¹¶ä»¥è‡ªç›‘ç£æ–¹å¼å­¦ä¹ å®ä½“/å…³ç³»åµŒå…¥ï¼Œå…¶ä¸­åº”ç”¨äº†è´Ÿé‡‡æ ·[4]ã€‚æ­¤å¤–ï¼Œæ ¹æ®è¯„åˆ†å‡½æ•°çš„è®¾è®¡ï¼ŒåŸºäºåµŒå…¥çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªå­ç±»åˆ«ï¼š

1. (1) åŸºäºå¹³ç§»çš„æ–¹æ³•å¦‚TransE [4]å’ŒRotatE [43]
2. [(2) å¼ é‡åˆ†è§£æ–¹æ³•å¦‚DistMult [56]å’ŒComplEx [46]
3. [(3) åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•å¦‚ConvE [38]ã€‚

åŸºäºåµŒå…¥çš„çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹æ³•å­¦ä¹ äº†ç”¨äºä¸‰å…ƒç»„åŒºåˆ†æ€§èƒ½åŠ›ä½†å¿½ç•¥äº†çŸ¥è¯†å›¾è°±ä¸­æ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨å¾ã€‚

#### PLM-based KGC

æ­¤å¤–ï¼ŒåŸºäºPLMçš„æ–¹æ³•å°†KGCè§†ä¸ºæ–‡æœ¬ä»»åŠ¡ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERT [12]ï¼‰æ¥è€ƒè™‘ã€‚å®ä½“å’Œå…³ç³»çš„ç®€çŸ­æ–‡æœ¬æè¿°è¢«ç»„ç»‡æˆè¾“å…¥åºåˆ—ï¼Œå¹¶ç”±PLMsç¼–ç ã€‚KG-BERT [58]æ˜¯ç¬¬ä¸€ä¸ªå°†KGCå»ºæ¨¡ä¸ºäºŒå…ƒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„åŸºäºPLMçš„æ–¹æ³•ã€‚åç»­å¦‚MTL-KGC [21]å’ŒStAR [48]é€šè¿‡å¼•å…¥æ›´å¤šè®­ç»ƒä»»åŠ¡ï¼ˆå¦‚å…³ç³»åˆ†ç±»å’Œä¸‰å…ƒç»„æ’åºï¼‰ä»¥åŠæ›´å¤æ‚çš„ä¸‰å…ƒç»„ç¼–ç ç­–ç•¥è¿›ä¸€æ­¥æ”¹è¿›äº†KG-BERTã€‚PKGC [28]åˆ©ç”¨æ‰‹åŠ¨æç¤ºæ¨¡æ¿æ•è·ä¸‰å…ƒç»„è¯­ä¹‰ã€‚å…¶ä»–æ–¹æ³•å¦‚KGT5 [37]å’ŒKG-S2S [7]åœ¨ä½¿ç”¨ç±»ä¼¼T5[35]è¿™æ ·çš„ç¼–ç å™¨-è§£ç å™¨PLMsè¿›è¡Œåºåˆ—åˆ°åºåˆ—èŒƒå¼ä¸­å¯¹ç”Ÿæˆå¼KGC[61]è¿ˆå‡ºäº†ä¸€æ­¥ã€‚åŸºäºPLMçš„æ–¹æ³•åˆ©ç”¨äº†PLMçš„å¼ºå¤§åŠŸèƒ½ï¼Œä½†å°†è®­ç»ƒè¿‡ç¨‹è½¬åŒ–ä¸ºåŸºäºæ–‡æœ¬å­¦ä¹ ï¼Œéš¾ä»¥æ•æ‰çŸ¥è¯†å›¾è°±ä¸­å¤æ‚ç»“æ„ä¿¡æ¯ã€‚



### Notations

LLMs serves as a text decoder

The input textual sequence S of the model M consists of several parts: the **instruction prompt I**, **the triple prompt X**, and **the optional auxiliary demonstration prompt U**.

The triple prompt X contains the **textual information** about the triples that need to be processed, which can be denoted as:

<img src="./assets/CleanShot 2024-03-18 at 15.11.51@2x.png" alt="CleanShot 2024-03-18 at 15.11.51@2x" style="zoom:50%;" />

- D is the description set of each entity and relation. We denote D (ğ‘’), D (ğ‘Ÿ ) as the short textual description of each entity ğ‘’ âˆˆ E and each relation ğ‘Ÿ âˆˆ R

objectiveï¼š**triple classification**

### 3.3 Instruction Tuning Approach

how to incorporate the KG information into IT approaches.

#### 3.3.1 vanilla instruction tuning

To train the model M, the input sequence is organized as $Sğ‘–ğ‘¡ = Iğ‘–ğ‘¡ âŠ• X âŠ• Ağ‘–ğ‘¡$ . where $Ağ‘–ğ‘¡$ is the predicted answer of the training data. The model $M $ is fine-tuned with the next word prediction task [67] which is a universal approach to training LLMs. The training objective can be formulated as:

<img src="./assets/CleanShot 2024-03-18 at 15.31.57@2x.png" alt="CleanShot 2024-03-18 at 15.31.57@2x" style="zoom:50%;" />

- where ğ‘ ğ‘– (ğ‘– = 1, 2, . . . , |Sğ‘–ğ‘¡ |) represents the textual tokens of the input sequence Sğ‘–ğ‘¡

In the inference stage, the model M is employed to predict the answer Ağ‘–ğ‘¡ of the test data like Equation 2. 

<img src="./assets/CleanShot 2024-03-18 at 15.32.55@2x.png" alt="CleanShot 2024-03-18 at 15.32.55@2x" style="zoom:50%;" />

- input sequence Sğ‘§ğ‘ ğ‘Ÿ .
- Iğ‘§ğ‘ ğ‘Ÿ is the instruction template for ZSR

Besides, negative sampling [28] is also applied as training KG only consists of positve triples.

**vanilla itçš„ç¼ºç‚¹ï¼š**Vanilla IT only fine-tunes the LLM to learn the knowledge in the single triple to discriminate. Such an approach makes it difficult to fully utilize the rich semantics present in a KG and the model performance is limited

#### 3.3.2 Structural-aware Instruction Tuning.

**local structural information of the entities is added into the input sequence in the form of neighborhood triples.**

To incorporate such KG information during the finetuning stage, we achieve this goal by adding the neighborhood descriptions of the input triple.

Specifically, we can sample the neighborhoods of the head â„ and tail ğ‘¡ and put the textual descriptions of neighborhood triples in the demonstration prompt **Uğ‘–ğ‘¡** .

<img src="./assets/CleanShot 2024-03-18 at 15.37.10@2x.png" alt="CleanShot 2024-03-18 at 15.37.10@2x" style="zoom:50%;" />

## 4 KNOWLEDGE PREFIX ADAPTER FOR LLM-BASED KGC

However, **representing the KG structural information in the form of text is not a good choice**, which may bring in more invalid or redundant information to the prompt.

é—®é¢˜ï¼š

1. é•¿ä¸Šä¸‹æ–‡å½±å“èƒ½åŠ›
2. éš¾æ‰¾åˆ°KGç»“æ„ä¸­çš„å¯¹äºtriple classification å…³é”®çš„ä¿¡æ¯

**K**n**o**wledge **P**refix **A**dapter (**KoPA** for short)

<img src="./assets/CleanShot 2024-03-18 at 15.44.12@2x.png" alt="CleanShot 2024-03-18 at 15.44.12@2x" style="zoom:50%;" />

1. **extract the structural information of entities and relations from the KG through structural embedding pre-training**
2. **inject this structural information through a structural prefix adapter into the input sequence $\mathcal{S}$**
   1. **The LLM $\mathcal{M}$ is further fine-tuned with the structural-injected sequence.**

### 4.1 Structural Embedding Pre-training

KoPA extracts the structural information of the entities and relations by **self-supervised structural embedding pre-training**

For each entity ğ‘’ âˆˆ E and each relation ğ‘Ÿ âˆˆ R, we learn a structural embedding $ğ’† âˆˆ \mathbb{R}^{ğ‘‘_ğ‘’}$ , $ğ’“ âˆˆ \mathbb{R}^{ğ‘‘_ğ‘Ÿ}$ respectively, where ğ‘‘ğ‘’, ğ‘‘ğ‘Ÿ are the embedding dimensions.

We encode the KG structural information in the embeddings and **further adapt them into the textual representation space of LLMs**.



score function F (â„, ğ‘Ÿ, ğ‘¡) to measure the plausibility of the triple (â„, ğ‘Ÿ, ğ‘¡).

self-supervised pre-training objective by negative sampling:

<img src="./assets/CleanShot 2024-03-18 at 16.04.10@2x.png" alt="CleanShot 2024-03-18 at 16.04.10@2x" style="zoom:50%;" />

where ğ›¾ is the margin, ğœ is the sigmoid activation function and (â„â€² ğ‘–,ğ‘Ÿâ€² ğ‘–,ğ‘¡â€² ğ‘– )(ğ‘– = 1, 2, . . . , ğ¾) are ğ¾ negative samples [4] of (â„, ğ‘Ÿ, ğ‘¡). The weight ğ‘ğ‘– is the self-adversarial weights proposed in RotatE.



By minimizing such a pre-training loss, the structural embeddings of each entity and relation are optimized to fit all its relative triples thus the KG structural information such as subgraph structure and relational patterns is captured in the embeddings.è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜åœ¨è®¸å¤šåŸºäºåµŒå…¥çš„KGCæ–¹æ³•ä¸­æ˜¯æœ‰æ•ˆçš„[4, 43, 46, 56]ï¼Œåœ¨æ—©æœŸä¹Ÿè¢«ç§°ä¸ºåˆ†å¸ƒå¼è¡¨ç¤º[17]ã€‚

### 4.2 Knowledge Prefix Adapter

After structural embedding pre-training, we could obtain the structural embeddings (ğ’‰, ğ’“, ğ’•) of a triple (â„, ğ‘Ÿ, ğ‘¡) where the KG structural information is encoded in.

ç»“æ„çš„embeddingæ˜¯åœ¨ä¸LLMçš„æ–‡æœ¬è¡¨ç¤ºçš„è¯­æ„ç©ºé—´ä¸åŒçš„è¯­æ„ç©ºé—´ä¸­å­¦ä¹ çš„ï¼Œå› æ­¤LLMä¸èƒ½ç›´æ¥ç†è§£ï¼Œ**å› æ­¤ç”¨knowledge prefix adapter$\mathcal{P}$æ¥æŠŠç»“æ„çš„embeddingæ˜ å°„åˆ°textual token representation space of M**

Specifically speaking, the structural embeddings are converted to several virtual knowledge tokens K by P:

<img src="./assets/CleanShot 2024-03-18 at 16.14.52@2x.png" alt="CleanShot 2024-03-18 at 16.14.52@2x" style="zoom:50%;" />

In practice, the adapter P would be a simple projection layerã€MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.ã€‘

Then we put K in the front of the original input sequence S serving as a prefix of the instruction and triple prompt $S_{ğ‘˜ğ‘ğ‘} = K âŠ• I_{ğ‘–ğ‘¡} âŠ• X$  è¿™æ ·ï¼Œç”±äº**ä»…è§£ç å™¨LLMä¸­çš„å•å‘æ³¨æ„åŠ›**ï¼Œæ‰€æœ‰ä»¥ä¸‹æ–‡æœ¬æ ‡è®°éƒ½å¯ä»¥çœ‹åˆ°å‰ç¼€Kï¼Œé€šè¿‡è¿™æ ·åšï¼Œæ–‡æœ¬æ ‡è®°å¯ä»¥å•å‘å…³æ³¨è¾“å…¥ä¸‰å…ƒç»„çš„ç»“æ„åµŒå…¥ã€‚è¿™ç§ç»“æ„æ„ŸçŸ¥æç¤ºå°†åœ¨å¾®è°ƒå’Œæ¨ç†æœŸé—´ä½¿ç”¨ã€‚

**During training**ï¼š

- froze the pre-trained structural embeddings
- The adapter is optimized to learn the mapping from structural knowledge toward textual representation and will have the generalization to new triples in the inference stage, which will benefit the textual description and provide the triple information from another perspective to make enhanced predictions.

### 4.3 Complexity Analysis

<img src="./assets/CleanShot 2024-03-18 at 16.22.10@2x.png" alt="CleanShot 2024-03-18 at 16.22.10@2x" style="zoom:50%;" />

- length of virtual tokens generated by the structural prefix adapter is fixed to 3 for head/relation / tail respectively.
- ZSRï¼ˆzero-shot reasoningï¼‰

## 5 EXPERIMENTS

#### dataset

<img src="./assets/CleanShot 2024-03-18 at 16.25.09@2x.png" alt="CleanShot 2024-03-18 at 16.25.09@2x" style="zoom:50%;" />

#### settings

- **For embedding-based KGC methods**:we reproduce the results with OpenKE we set the embedding dimension ğ‘‘ğ‘’ = ğ‘‘ğ‘Ÿ = 512 and sample ğ¾ = 32 negative samples during training. The margin ğ›¾ is tuned among {0, 4, 6, 8, 12}. After training KGC models, we search for the best classification score threshold on the validation set for test data following the traditional setting
- **For PLM-based methods**, the backbone model for PLM-based KGC methods is BERT.We **fine-tune the KG-BERT** according to the official code implementation
- **For all LLM-based methods**, we employ Alpaca-7B [44] as the LLM backbone.

#### main result

<img src="./assets/CleanShot 2024-03-18 at 16.25.23@2x.png" alt="CleanShot 2024-03-18 at 16.25.23@2x" style="zoom:50%;" />

#### transferbility

<img src="./assets/CleanShot 2024-03-18 at 16.25.59@2x.png" alt="CleanShot 2024-03-18 at 16.25.59@2x" style="zoom:50%;" />

#### ablation

<img src="./assets/CleanShot 2024-03-18 at 16.27.22@2x.png" alt="CleanShot 2024-03-18 at 16.27.22@2x" style="zoom:50%;" />

- two part ablation study
  - The first part is designed to verify the **effectiveness of structural embedding**
    - removing the structural embeddings or replacing them with random initialized embeddings both lead to *performance decline*
    - the model is *compatible with different types of structural embeddings*.
      - However, the performance gain depends on whether the embedding was originally powerful in the triple classification task or not.
      - TransE [4] å’Œ RotatE [43] æ˜¯æ¯” DistMult [56] å’Œ ComplEx [46] æ›´å¥½çš„åŸºäºåµŒå…¥çš„çŸ¥è¯†å›¾è°±æ¨¡å‹ã€‚è¿™è¡¨æ˜è¯­ä¹‰ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯æ˜¯æ€§èƒ½æå‡çš„å…³é”®ï¼Œè€Œ KoPA å……åˆ†åˆ©ç”¨äº†å®ƒã€‚
    - 
  - the second part is designed to verify the **effectiveness of prefix adapter**.
    - putting the virtual knowledge tokens generated by the adapter **in the middle (infix)** or **in the last (suffix)** of the input sequence will also *decrease the performance*.
    - **putting tokens in the front of the sequence will make all the text pay attention to them as LLMs are usually decoder-only architectures with unidirectional self-attention.**
      - the LLM can make a better decision with the structural embeddings that fully interact with the text.