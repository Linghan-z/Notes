# LABEL-FREE NODE CLASSIFICATION ON GRAPHS WITH LARGE LANGUAGE MODELS (LLMS)

> Key Points：label-free node classification、使用llm annotate
>
> - 聚类方法结合其他方法进行node selection
> - 用llm对选择的node进行标注
> - 筛选掉效果差的
> - 使用GNN进行任务

## Research Questions

- actively select nodes for LLMs to annotate and consequently enhance the GNN training
- leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost

## Methods

**LLM-GNN**：label-free node classification on graphs with LLMs pipeline

<img src="./assets/CleanShot 2024-04-21 at 20.14.34@2x.png" alt="CleanShot 2024-04-21 at 20.14.34@2x" style="zoom:50%;" />

1. The active ==**node selection**== phase is to find a candidate node set for LLM annotation.
   - difficulty-aware
     - the accuracy of annotations generated by LLMs is closely related to the <u>*clustering density*</u> of nodes.
     - k-means clustering
       - <u>*nodes closer to cluster **centers** typically exhibit better annotation quality, which indicates lower annotation difficulty*</u>
       - 离中心越近越简单
     - <img src="./assets/CleanShot 2024-04-21 at 21.02.44@2x.png" alt="CleanShot 2024-04-21 at 21.02.44@2x" style="zoom:40%;" /><img src="./assets/CleanShot 2024-04-21 at 21.03.00@2x.png" alt="CleanShot 2024-04-21 at 21.03.00@2x" style="zoom:50%;" />
     - traditional active node selection<img src="./assets/CleanShot 2024-04-21 at 21.04.27@2x.png" alt="CleanShot 2024-04-21 at 21.04.27@2x" style="zoom:50%;" />
       - <img src="./assets/CleanShot 2024-04-21 at 21.04.58@2x.png" alt="CleanShot 2024-04-21 at 21.04.58@2x" style="zoom:40%;" />
     - r ：rank
     - <img src="./assets/CleanShot 2024-04-21 at 21.02.23@2x.png" alt="CleanShot 2024-04-21 at 21.02.23@2x" style="zoom:50%;" />
     - 确保所选注释节点有效地为模型学习过程做出贡献，同时考虑到准确标注某些节点可能涉及的潜在挑战和复杂性。该方法最终旨在增强注释的质量和多样性，从而提高经过训练的模型在节点分类任务中的表现。
2. With the selected node set, utilize the strong zero-shot ability of LLMs to ==**annotate**== those nodes with confidence-aware prompts.
   - strategies:
     1. Vanilla (zero-shot)：directly asking for confidence
     2. reasoning-based prompts：CoT、multi-step reasoning
     3. TopK prompt：generate the top K possible answers, select the most probable
     4. Consistency-based prompt（Most voting）：queries LLMs multiple times
     5. Hybrid prompt：3+4
3. The optional post-filtering stage aims to ==**remove low-quality annotations**==.
   - 在保留多样性和label的分布的情况下
   - change of entropy (COE)：entropy change of labels when removing a node from the selected set.
     - 小表明改变结构，可能会影响模型的性能
   - 结合fconf(vi)（confidence score），balance diversity and annotation quality
   - <img src="./assets/CleanShot 2024-04-21 at 21.11.27@2x.png" alt="CleanShot 2024-04-21 at 21.11.27@2x" style="zoom:50%;" />
   - 
4. With the filtered high-quality annotation set ==**train GNN models**== on selected nodes and their annotations.

## Experiments & Results

### dataset

<img src="./assets/CleanShot 2024-04-22 at 10.47.20@2x.png" alt="CleanShot 2024-04-22 at 10.47.20@2x" style="zoom:50%;" />

### 研究问题

1. active selection, post-filtering, loss function 的影响

   <img src="./assets/CleanShot 2024-04-22 at 10.47.52@2x.png" alt="CleanShot 2024-04-22 at 10.47.52@2x" style="zoom:50%;" />

   - DA-表示其他方法结合C-Density（difficulty-aware）
   - PS-结合confidence and COE-based selections
   - “-W”-weighted cross entropy loss
   - 结果：
     - post-filtering strategy有效果
     - C-Density-based selection可以提升 annotation quality ，但是仅使用它效果不好，因为主动选择带来的标签不平衡。例如，我们检查了 PUBMED 的选定节点，并发现所有注释都属于同一类别。我们进一步发现，调整 C-Density 的聚类中心数量可以在多样性和注释质量之间取得平衡，较大的 K 值可以缓解类别不平衡问题。
     - weighted cross entropy loss 一般更有效
   - 总结：
     - （1）基于Featprop的方法可以在不同数据集上高效地实现有希望的性能；
     - （2）比较DA和PS，由于我们不需要LLMs生成置信度并且可能使用更简单的提示，因此DA成本较低。PS通常可以获得更好的性能。在大规模数据集上，PS通常会获得更好的结果。

2. performance and cost compare

   <img src="./assets/CleanShot 2024-04-22 at 10.59.02@2x.png" alt="CleanShot 2024-04-22 at 10.59.02@2x" style="zoom:50%;" />

   - (1) Zero-shot node classification method: SES, TAG-Z
   - (2) Zero-shot classification models for texts: BART-large-MNLI
   - (3) Directly using LLMs for predictions: LLMs-as-Predictors

3. budget 影响

   - <img src="./assets/CleanShot 2024-04-22 at 11.02.03@2x.png" alt="CleanShot 2024-04-22 at 11.02.03@2x" style="zoom:50%;" />
     - (1) with the increase in the budget, the performance tends to increase gradually. 
     - (2) unlike using ground truth, the performance growth is relatively limited as the budget increases. It suggests that there exists a trade-off between the performance and the cost in the real-world scenario.

4. llm annotation 和 ground truth labels 的比较

   <img src="./assets/CleanShot 2024-04-22 at 13.17.31@2x.png" alt="CleanShot 2024-04-22 at 13.17.31@2x" style="zoom:50%;" />

LLMs的注释与合成噪声标签呈现完全不同的训练动态。LLMs的注释过拟合程度远低于合成噪声标签。



