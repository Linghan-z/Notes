# Can GNN be Good Adapter for LLMs?

Thus, this paper explores how to utilize LLMs to model TAGs.

> TAG-text-attribute graph (the nodes in graphs have textual features) -> KG

æå‡º==**GraphAdapter**== åˆ©ç”¨GNN+LLMs

- The entire framework is trained using auto-regression on node text (next token prediction).
- è®­ç»ƒä¹‹åå¯ä»¥é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒ

KGä¸­çš„ç»“æ„ä¿¡æ¯å’ŒèŠ‚ç‚¹çš„æ–‡æœ¬ä¿¡æ¯çš„è”ç³»ï¼šå›¾å¯ä»¥é€šè¿‡ç»“æ„é‚»è¿‘æ€§ï¼ˆstructural proximityï¼‰.æ¥è¡¥å……èŠ‚ç‚¹ä¸Šçš„æ–‡æœ¬å±æ€§

- åˆ©ç”¨GNNæå–KGä¸­çš„æ–‡æœ¬ä¿¡æ¯å’Œç»“æ„ä¿¡æ¯

GNN+LMsçš„æŒ‘æˆ˜ï¼šæ¶ˆæ¯ä¼ é€’æœºåˆ¶å¸¦æ¥çš„æå¤§é¢å¤–è®¡ç®—æˆæœ¬

**==GraphAdapter==** offers several advantages:

- Lightweight
- Language-aware graph pre-trainingï¼šUsing <u>*language to supervise the modeling of graph structur*</u>e, which can help LLMs comprehend <u>*both textual and structural information*</u>.
- Convenient tuningï¼šOnce a graph-specific adapter is <u>*pretrained*</u>, it can be fine-tuned for <u>*multiple downstream tasks.*</u>

æ¨¡å‹çš„ç»†èŠ‚ä»‹ç»ï¼š

- ä¸ºäº†æ•è·å›¾çš„æ•°æ®åˆ†å¸ƒï¼Œæˆ‘ä»¬é‡‡ç”¨èŠ‚ç‚¹æ–‡æœ¬ä¸ŠLLMsçš„å‚æ•°é«˜æ•ˆè°ƒæ•´
  - GNN is the tuning parameter,
  - helps reduce the distribution discrepancy between the pre-training corpus and target data.å‡å°‘é¢„è®­ç»ƒè¯­æ–™åº“å’Œç›®æ ‡æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚
- åªåœ¨transformerçš„æœ€åä¸€å±‚åˆ©ç”¨GNN adapter
- æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¥è‡ªGNNé€‚é…å™¨å’ŒLLMsçš„é¢„æµ‹logitsæ‰§è¡Œå‡å€¼æ± åŒ–ï¼Œç„¶åä¼˜åŒ–å®ƒä»¬çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹çš„æœ€ç»ˆç»“æœï¼Œè¿™å¯ä»¥å¸®åŠ©é€‚é…å™¨æ›´å¤šåœ°å…³æ³¨ä¸å›¾ç›¸å…³çš„token 

**PLMs with Promptsï¼š**Since the last token is used to predict the next token distribution in the pre-training stage, it can naturally combine the inserted prompt information into the original sentence and extract the prompt-related semantics.

## METHOD

<img src="./assets/CleanShot 2024-04-10 at 10.02.14@2x.png" alt="CleanShot 2024-04-10 at 10.02.14@2x" style="zoom:50%;" />

during the pre-training processï¼š

1. GNN models the node structure information
2. integrates the structural information with the corresponding context hidden-states modeled by PLM
3. predicts the next token.

**Align GNN with the language model.**ï¼šGNNå¾—åˆ°çš„èŠ‚ç‚¹è¡¨ç¤ºä¸è¯­è¨€æ¨¡å‹å»ºæ¨¡çš„ä¸åŒè¡¨ç¤ºä¸æ–­åœ°ç»“åˆèµ·æ¥è¿›è¡Œæ¨ç†ï¼Œæ•´ä¸ªè¿‡ç¨‹è‡ªç„¶åœ°å°†ä¸¤è€…å¯¹é½

**Enhance GNN in modeling graph structure.**ï¼šåœ¨æ•´ä¸ªé¢„è®­ç»ƒé˜¶æ®µï¼Œæ–‡æœ¬æ•°æ®ä¸­çš„è¯­ä¹‰ä¿¡æ¯ç›‘ç£GNNå¯¹å›¾çš„ç»“æ„ä¿¡æ¯è¿›è¡Œå»ºæ¨¡

**Better understanding the semantics in TAG.**ï¼šGraphAapter can learn how to combine LLM and GNN to model the semantic information on TAG.

### 4.2 Pre-training on TAGs

#### Pipeline of pre-training

- ç»™å®šèŠ‚ç‚¹iå’Œå®ƒçš„æ–‡æœ¬ä¿¡æ¯$\mathbb{S}_ğ‘– = {ğ‘ _{ğ‘–,0}, ..., ğ‘ _{ğ‘–,ğ¿_ğ‘–} }$
- ä½¿ç”¨$\mathbb{S_i}$ç›‘ç£
  - å¯¹äºk-thä¸ªtokenï¼Œ
    - é¦–å…ˆæŠ½å–å‰k-1ä¸ªtokens$\mathbb{S}_ğ‘– = {ğ‘ _{ğ‘–,0}, ..., ğ‘ _{ğ‘–,k-1} }$
    - GNNå­¦ä¹ ç¬¬kä¸ªèŠ‚ç‚¹çš„ç»“æ„ä¿¡æ¯$z_i$
    - ç»“æ„ä¿¡æ¯å’Œå‰k-1ä¸ªtokençš„ä¿¡æ¯ç»“åˆèµ·æ¥æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    - groundtruthï¼š$s_{i,k}$

#### Structural representation:

- ä½¿ç”¨GNNæŠ½å–zi
  - massage-passing frameworkçš„GNN
  - <img src="./assets/CleanShot 2024-04-10 at 10.26.23@2x.png" alt="CleanShot 2024-04-10 at 10.26.23@2x" style="zoom:50%;" />
  - xi0:åˆå§‹èŠ‚ç‚¹ Aï¼šé‚»æ¥çŸ©é˜µ

#### Context hidden-states.

- ä½¿ç”¨é¢„è®­ç»ƒçš„**Transformer** æ¥encode $S_{i,k}$
  - <img src="./assets/CleanShot 2024-04-10 at 10.34.30@2x.png" alt="CleanShot 2024-04-10 at 10.34.30@2x" style="zoom:50%;" />
  - **Transformer** é¢„è®­ç»ƒä¸”å†»ç»“
  - hikæ˜¯$S_{i,k}$ çš„context hidden state
  - in the pretraining stage of PLM, hğ‘–,ğ‘˜ is directly used to predict the next token, so hğ‘–,ğ‘˜ contains both the context information and a certain of PLMsâ€™ prediction result.

#### Fusion block:

- èåˆç»“æ„ä¿¡æ¯å’Œè¯­æ„ä¿¡æ¯
  - <img src="./assets/CleanShot 2024-04-10 at 10.37.14@2x.png" alt="CleanShot 2024-04-10 at 10.37.14@2x" style="zoom:50%;" />
  - The Fusion(*) function is trainable with parameters $Î˜_{fuse}$ 
  - concat $h_{i,k}$ å’Œ $z_i$
  - ç»™MLP

#### Residual connection

- not every tokenâ€™s prediction requires the graph structure.
- reuse hik
  - åˆ†åˆ«è®¡ç®—LMçš„è¯­æ„çš„æ¦‚ç‡ä»¥åŠç»“æ„+è¯­æ„çš„æ¦‚ç‡
  - <img src="./assets/CleanShot 2024-04-10 at 10.46.26@2x.png" alt="CleanShot 2024-04-10 at 10.46.26@2x" style="zoom:50%;" />
  - $\sigma$ï¼šsoftmax

#### Optimization

- cross-entropy loss
  - <img src="./assets/CleanShot 2024-04-10 at 10.51.34@2x.png" alt="CleanShot 2024-04-10 at 10.51.34@2x" style="zoom:50%;" />

#### GNN as Adapter:

In the whole pre-training stage, the GNN <u>*combines*</u> with the frozen LMâ€™s hidden states outputted from the transformer block. The <u>*combined hidden states are then input into the PLMâ€™s prediction head*</u>. Thus, the GNN acts as an adapter, altering the language modelâ€™s predictions. Since the hidden states outputted by the transformer block can be pre-processed and stored in advance. Therefore, the entire training process <u>*only requires training the GNN*</u>. Therefore, GraphAdpater can efficiently pre-train based on different scales of PLMs.

### 4.3 Fine-tuning with Prompts

GraphAdapter is pre-trained by token-level semantic understanding tasks

**prompt-aware fine-tuning.**

- æ›´å¥½åœ°åˆ©ç”¨learned knowledge of GraphAdapter

  - inserts prompts in textual data to get task-specific sentence embedding of each node.

  - PromptsæŠŠä¸‹æ¸¸ä»»åŠ¡è½¬åŒ–ä¸ºnext token prediction

  - given textual data $\mathbb{S_ğ‘–}$ of node $ğ‘–$

    1. combine a sequence of tokens with task-specific prompts behind textual data

    - $\mathbb{S}_{i|\mathbb{P}} = [\mathbb{S}_i, \mathbb{P}]$

    2. get its sentence hidden states $h_{i|\mathbb{P}}$ through the transformer of PLM.

    3. The resulting hidden state is then **fused with the nodeâ€™s structural representation** as node representation in a specific downstream task.

    - <img src="./assets/CleanShot 2024-04-10 at 11.05.27@2x.png" alt="CleanShot 2024-04-10 at 11.05.27@2x" style="zoom:50%;" />

  - This node representation can be used in various tasks.

    - node classification:append a new linear transformation to output the result
      - <img src="./assets/CleanShot 2024-04-10 at 11.07.12@2x.png" alt="CleanShot 2024-04-10 at 11.07.12@2x" style="zoom:50%;" />
      - In fine-tuning stage, the whole parameters$\{\Theta_g, \Theta_{fuse}, \theta_{new}\}$ in GraphAdapter are trainable.

## 5 EXPERIMENT

GNN:GraphSAGE [11]

- **Q1: How well is GraphAdapter in modeling TAGs?** 
  - **A1: GraphAdapter can effectively model TAGs and surpass current state-of-the-art baselines on node classification tasks.**
    - <img src="./assets/CleanShot 2024-04-10 at 13.40.30@2x.png" alt="CleanShot 2024-04-10 at 13.40.30@2x" style="zoom:50%;" />
      1. Frozen LLMs are effective on TAGs.
         - prompts can bring a 0.42% improvement on average for LLM, but they could not improve the performance of LM.
      2. Directly fusing GNN and LLM results in unstable improvements.
         - *GraphAdapter (w/o Pre)* only adds one fusion component to fuse the semantic representation from the LM and structural representation from the GNN.
           - training samples may have an impact on GNNs to understand and effectively incorporate the representations inferred by LLMs with prompts.
      3. GraphAdapter can effectively combine GNN and LLM, surpassing existing state-of-the-art baselines in terms of performance.
- **Q2: Whether GraphAdapter can adapt to other PLMs?** 
  - **A2: GraphAdapter can be effectively pre-trained based on RoB-ERTa, GPT-2, and Llama 2, resulting in performance improvements.**
    - <img src="./assets/CleanShot 2024-04-10 at 13.44.31@2x.png" alt="CleanShot 2024-04-10 at 13.44.31@2x" style="zoom:50%;" />
      - **GraphAdapter is a general and scalable method.**
- **Q3: Are all components comprising GraphAdapter valid?** 
  - **A3: As Table 4 shows, removing any component of GraphAdapter results in performance drops.**
    - <img src="./assets/CleanShot 2024-04-10 at 13.49.51@2x.png" alt="CleanShot 2024-04-10 at 13.49.51@2x" style="zoom:50%;" />
      - removing the residual learning (â€œw/o Res Labelâ€ that is stated in Equation 12) leads to a 1.02% drop (more than removing pre-training), suggesting that training GNNs directly on all text may introduce excessive noise and hurt performance.(æ®‹å·®è¿æ¥)
- **Q4: What exactly does GraphAdapterâ€™s pre-training learn?** 
  1. **GNN can obtain stronger expressive power through pre-training.**
     - <img src="./assets/CleanShot 2024-04-10 at 13.52.53@2x.png" alt="CleanShot 2024-04-10 at 13.52.53@2x" style="zoom:50%;" />
     - GNNs are training their ability to model the graph structure during pre-training
  2. **Fusion block is learning how to fuse the knowledge from the language model and GNN during pre-training**
     - <img src="./assets/CleanShot 2024-04-10 at 13.53.59@2x.png" alt="CleanShot 2024-04-10 at 13.53.59@2x" style="zoom:50%;" />
  3. **Graph structure is the basis of pre-training.**
     - <img src="./assets/CleanShot 2024-04-10 at 14.00.45@2x.png" alt="CleanShot 2024-04-10 at 14.00.45@2x" style="zoom:50%;" />
       - the MLP-based GraphAdapter shows no significant change before and after pre-training (average improvement of 0.19%), and even decreases in performance on Instagram and Reddit (drops of 0.05% and 0.62% respectively).
- Q5: How efficient is GraphAdapter?

## 6 CONCLUSION

This paper proposes GraphAdapter to harness LLMs for TAGs without fine-tuning. A GNN adapter is trained to reduce LLM next-word errors on node texts. This adapts LLMs for graphs efficiently. Across node classification tasks, GraphAdapter improves accuracy by 5% over baselines. We validate with RoBERTa, GPT-2, and Llama 2, efficiently leveraging LLMs for interconnected text-graph data.
